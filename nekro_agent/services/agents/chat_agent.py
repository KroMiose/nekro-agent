import asyncio
import os
import time
from typing import List, Optional, Union

import weave
from miose_toolkit_llm import (
    BaseScene,
    BaseStore,
    ModelResponse,
    Runner,
)
from miose_toolkit_llm.clients.chat_openai import (
    OpenAIChatClient,
)
from miose_toolkit_llm.components import (
    TextComponent,
)
from miose_toolkit_llm.creators.openai import (
    AiMessage,
    OpenAIPromptCreator,
    SystemMessage,
    UserMessage,
)
from miose_toolkit_llm.exceptions import (
    ResolveError,
    SceneRuntimeError,
)
from miose_toolkit_llm.tools.tokenizers import TikTokenizer

from nekro_agent.core import logger
from nekro_agent.core.config import ModelConfigGroup, config
from nekro_agent.core.os_env import PROMPT_LOG_DIR
from nekro_agent.models.db_chat_channel import DBChatChannel
from nekro_agent.models.db_chat_message import DBChatMessage
from nekro_agent.schemas.chat_message import ChatMessage
from nekro_agent.services.chat import chat_service
from nekro_agent.services.sandbox.executor import CODE_RUN_ERROR_FLAG, limited_run_code
from nekro_agent.systems.message.push_bot_msg import push_system_message

from .components.chat_history_cmp import ChatHistoryComponent
from .components.chat_ret_cmp import (
    ChatResponseResolver,
    ChatResponseType,
    check_missing_call_response,
    check_negative_response,
)

OPENAI_BASE_URL = os.environ.get("OPENAI_BASE_URL")
OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY")


class ChatScene(BaseScene):
    """åŸºæœ¬å¯¹è¯åœºæ™¯ç±»"""

    class Store(BaseStore):
        """åœºæ™¯æ•°æ®æºç±»"""

        chat_key: str = ""
        chat_preset: str = config.AI_CHAT_PRESET_SETTING
        one_time_code: str = ""


@weave.op(name="agent_run")
async def agent_run(
    chat_message: ChatMessage,
    addition_prompt_message: Optional[List[Union[UserMessage, AiMessage]]] = None,
    retry_depth: int = 0,
):
    """ä»£ç†æ‰§è¡Œå‡½æ•°"""

    sta_timestamp = time.time()
    one_time_code = os.urandom(4).hex()  # é˜²æ­¢æç¤ºè¯æ³¨å…¥ï¼Œç”Ÿæˆä¸€æ¬¡æ€§éšæœºç 

    if not addition_prompt_message:
        addition_prompt_message = []

    logger.info(f"æ­£åœ¨æž„å»ºå¯¹è¯åœºæ™¯: {chat_message.chat_key}")
    if config.DEBUG_IN_CHAT:
        await chat_service.send_message(chat_message.chat_key, "[Debug] æ€è€ƒä¸­ðŸ¤”...")

    db_chat_channel: DBChatChannel = DBChatChannel.get_channel(chat_key=chat_message.chat_key)
    # logger.info(f"åŠ è½½å¯¹è¯åœºæ™¯é…ç½®: {db_chat_channel.get_channel_data().render_prompt()}")

    # 1. æž„é€ ä¸€ä¸ªåº”ç”¨åœºæ™¯
    scene = ChatScene()
    scene.store.set("chat_key", chat_message.chat_key)
    scene.store.set("one_time_code", one_time_code)

    # 2. æž„å»ºèŠå¤©è®°å½•ç»„ä»¶
    chat_history_component = (
        ChatHistoryComponent(scene)
        .bind(
            param_key="one_time_code",
            store_key="one_time_code",
            src_store=scene.store,
        )
        .bind(
            param_key="chat_key",
            store_key="chat_key",
            src_store=scene.store,
        )
    )
    record_sta_timestamp = int(time.time() - config.AI_CHAT_CONTEXT_EXPIRE_SECONDS)
    recent_chat_messages: List[DBChatMessage] = (
        DBChatMessage.sqa_query()
        .filter(
            DBChatMessage.send_timestamp >= record_sta_timestamp,
            DBChatMessage.chat_key == chat_message.chat_key,
        )
        .order_by(DBChatMessage.send_timestamp.desc())
        .limit(config.AI_CHAT_CONTEXT_MAX_LENGTH)
        .all()
    )[::-1][-config.AI_CHAT_CONTEXT_MAX_LENGTH :]
    # è¿‡é•¿çš„ç³»ç»Ÿæ¶ˆæ¯è¿‡æ»¤ï¼Œåªä¿ç•™æŒ‡å®šæ¡æ•°
    allow_long_system_msg_cnt = 2
    allow_long_system_length = 128
    for db_message in recent_chat_messages:
        if db_message.sender_bind_qq == "0" and len(db_message.content_text) > allow_long_system_length:
            setattr(db_message, "_mark_del", True)  # noqa: B010
    for idx in range(len(recent_chat_messages) - 1, -1, -1):
        if getattr(recent_chat_messages[idx], "_mark_del", False):  # noqa: B010
            allow_long_system_msg_cnt -= 1
            setattr(recent_chat_messages[idx], "_mark_del", False)  # noqa: B010
            if allow_long_system_msg_cnt <= 0:
                break
    recent_chat_messages = [m for m in recent_chat_messages if not getattr(m, "_mark_del", False)]

    for db_message in recent_chat_messages:
        chat_history_component.append_chat_message(db_message)
    logger.info(f"åŠ è½½æœ€è¿‘ {len(recent_chat_messages)} æ¡å¯¹è¯è®°å½•")

    # 3. æž„é€  OpenAI æç¤ºè¯
    prompt_creator = OpenAIPromptCreator(
        SystemMessage(
            TextComponent(
                "Base Character Stetting For You: {chat_preset}",
                src_store=scene.store,
            ),
            ChatResponseResolver.example(one_time_code),  # ç”Ÿæˆä¸€ä¸ªè§£æžç»“æžœç¤ºä¾‹
            sep="\n\n",  # è‡ªå®šä¹‰æž„å»º prompt çš„åˆ†éš”ç¬¦ é»˜è®¤ä¸º "\n"
        ),
        UserMessage(ChatResponseResolver.practice_question_1()),
        AiMessage(ChatResponseResolver.practice_response_1()),
        UserMessage(ChatResponseResolver.practice_question_2()),
        AiMessage(ChatResponseResolver.practice_response_2()),
        UserMessage(
            TextComponent(
                (
                    "Good, this is an effective response to a positive action. Next is a real user conversation scene\n\n"
                    f"{(await db_chat_channel.get_channel_data()).render_prompts()}\n"
                    "Current Chat Key: {chat_key}"
                ),
                src_store=scene.store,
            ),
            chat_history_component,
        ),
        *addition_prompt_message,
        # ç”Ÿæˆä½¿ç”¨çš„å‚æ•°
        temperature=0.3,
        presence_penalty=0.3,
        frequency_penalty=0.4,
    )

    # 4. ç»‘å®š LLM æ‰§è¡Œå™¨
    model_group: ModelConfigGroup = ModelConfigGroup.model_validate(config.MODEL_GROUPS[config.USE_MODEL_GROUP])
    scene.attach_runner(  # ä¸ºåœºæ™¯ç»‘å®š LLM æ‰§è¡Œå™¨
        Runner(
            client=OpenAIChatClient(
                model=model_group.CHAT_MODEL,
                api_key=model_group.API_KEY or OPENAI_API_KEY,
                base_url=model_group.BASE_URL or OPENAI_BASE_URL,
                proxy=model_group.CHAT_PROXY,
            ),  # æŒ‡å®šèŠå¤©å®¢æˆ·ç«¯
            tokenizer=TikTokenizer(model=model_group.CHAT_MODEL),  # æŒ‡å®šåˆ†è¯å™¨
            prompt_creator=prompt_creator,
        ),
    )

    # 5. èŽ·å–ç»“æžœä¸Žè§£æž
    for _ in range(config.AI_CHAT_LLM_API_MAX_RETRIES):
        try:
            logger.debug("å‘é€ç”Ÿæˆè¯·æ±‚...")
            scene_run_sta_timestamp = time.time()
            mr: ModelResponse = await scene.run()
            logger.debug(f"LLM è¿è¡Œè€—æ—¶: {time.time() - scene_run_sta_timestamp:.3f}s")
            break
        except Exception as e:
            logger.error(f"LLM API error: {e}")
            await asyncio.sleep(1)
    else:
        await chat_service.send_agent_message(chat_message.chat_key, "å“Žå‘€ï¼Œè¯·æ±‚æ¨¡åž‹å‘ç”Ÿäº†æœªçŸ¥é”™è¯¯ï¼Œç­‰ä¼šå„¿å†è¯•è¯•å§ ~")
        raise SceneRuntimeError("LLM API error: è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œåœæ­¢é‡è¯•ã€‚") from None

    # 6. éžæ³•å›žå¤æ£€æŸ¥
    if retry_depth < 2 and check_missing_call_response(mr.response_text):
        logger.warning(f"æ£€æµ‹åˆ°ä¸æ­£ç¡®è°ƒç”¨å›žå¤: {mr.response_text}ï¼Œæ‹’ç»ç»“æžœå¹¶é‡è¯•")
        if config.DEBUG_IN_CHAT:
            await chat_service.send_message(chat_message.chat_key, "[Debug] æ£€æµ‹åˆ°ä¸æ­£ç¡®è°ƒç”¨å›žå¤ï¼Œæ‹’ç»ç»“æžœå¹¶é‡è¯•...")
        addition_prompt_message.append(AiMessage(mr.response_text))
        addition_prompt_message.append(
            UserMessage(
                "[System Automatic Detection] Content suspected to be missing the correct response type was detected in your reply, such as need for a script execution but not using the 'script:>' prefix to specify the response type. Your answer must specify the correct processing branch by response type. If you think this is an error, please ** keep the previously agreed reply format ** and try again.",
            ),
        )
        await agent_run(chat_message, addition_prompt_message, retry_depth + 1)
        return

    if (not retry_depth) and check_negative_response(mr.response_text):
        logger.warning(f"æ£€æµ‹åˆ°æ¶ˆæžå›žå¤: {mr.response_text}ï¼Œæ‹’ç»ç»“æžœå¹¶é‡è¯•")
        if config.DEBUG_IN_CHAT:
            await chat_service.send_message(chat_message.chat_key, "[Debug] æ£€æµ‹åˆ°æ¶ˆæžå›žå¤ï¼Œæ‹’ç»ç»“æžœå¹¶é‡è¯•...")
        addition_prompt_message.append(AiMessage(mr.response_text))
        addition_prompt_message.append(
            UserMessage(
                "[System Automatic Detection] A suspected negative or invalid response is detected in your reply (such as asking for a meaningless wait or claiming to do something but not do anything). Your answers must be consistent with your words and deeds, no pretending behavior, and no meaningless promises. If you think this is an error, please ** keep the previously agreed reply format ** and try again.",
            ),
        )
        await agent_run(chat_message, addition_prompt_message, retry_depth + 1)
        return

    try:
        resolved_response: ChatResponseResolver = ChatResponseResolver.resolve(
            model_response=mr,
        )  # ä½¿ç”¨æŒ‡å®šè§£æžå™¨è§£æžç»“æžœ
        logger.debug("è§£æžå®Œæˆç»“æžœå®Œæˆ")
    except Exception as e:
        logger.error(f"è§£æžç»“æžœå‡ºé”™: {e}")
        raise ResolveError(f"è§£æžç»“æžœå‡ºé”™: {e}") from e

    # 7. æ‰§è¡Œå“åº”ç»“æžœ
    logger.debug(f"å¼€å§‹æ‰§è¡Œ {len(resolved_response.ret_list)} æ¡å“åº”ç»“æžœ")
    for ret_data in resolved_response.ret_list:
        await agent_exec_result(ret_data.type, ret_data.content, chat_message, addition_prompt_message, retry_depth)

    # 8. åé¦ˆä¸Žä¿å­˜æ•°æ®
    if config.SAVE_PROMPTS_LOG:
        current_strftime = time.strftime("%Y%m%d%H%M%S")
        logger.debug(f"ä¿å­˜å¯¹è¯è®°å½•: {current_strftime}")
        mr.save(
            prompt_file=f".temp/prompts/chat_prompt-{current_strftime}.txt",
            response_file=f".temp/prompts/chat_response-{current_strftime}.json",
        )
        logger.debug("å¦å­˜æœ€æ–°å¯¹è¯è®°å½•")
        mr.save(
            prompt_file=".temp/chat_prompt-latest.txt",
            response_file=".temp/chat_response-latest.json",
        )

    logger.info(f"æœ¬è½®å“åº”è€—æ—¶: {time.time() - sta_timestamp:.2f}s | To {chat_message.sender_nickname}")


async def agent_exec_result(
    ret_type: ChatResponseType,
    ret_content: str,
    chat_message: ChatMessage,
    addition_prompt_message: List[Union[UserMessage, AiMessage]],
    retry_depth: int = 0,
):
    if ret_type is ChatResponseType.TEXT:
        logger.info(f"è§£æžæ–‡æœ¬å›žå¤: {ret_content} | To {chat_message.sender_nickname}")
        await chat_service.send_agent_message(chat_message.chat_key, ret_content, record=True)
        return

    if ret_type is ChatResponseType.SCRIPT:
        if ret_content.endswith("\n```"):
            ret_content = ret_content[:-3]
        logger.info(f"è§£æžç¨‹å¼å›žå¤: ç­‰å¾…æ‰§è¡Œèµ„æº | To {chat_message.sender_nickname}")
        if config.DEBUG_IN_CHAT:
            await chat_service.send_message(chat_message.chat_key, "[Debug] æ‰§è¡Œç¨‹å¼ä¸­ðŸ–¥ï¸...")
        result: str = await limited_run_code(ret_content, from_chat_key=chat_message.chat_key)
        if result.endswith(CODE_RUN_ERROR_FLAG):  # è¿è¡Œå‡ºé”™æ ‡è®°ï¼Œå°†é”™è¯¯ä¿¡æ¯è¿”å›žç»™ AI
            err_msg = result[: -len(CODE_RUN_ERROR_FLAG)]
            addition_prompt_message.append(AiMessage(f"script:>\n{ret_content}"))
            if retry_depth < config.AI_SCRIPT_MAX_RETRY_TIMES - 1:
                addition_prompt_message.append(
                    UserMessage(
                        f"Code run error: {err_msg or 'No error message'}\nPlease maintain agreed reply format and try again.",
                    ),
                )
            else:
                addition_prompt_message.append(
                    UserMessage(
                        f"Code run error: {err_msg or 'No error message'}\nThe number of retries has reached the limit, you should give up retries and explain the problem you are experiencing.",
                    ),
                )
            logger.info(f"ç¨‹å¼è¿è¡Œå‡ºé”™: ...{err_msg[-100:]} | é‡è¯•æ¬¡æ•°: {retry_depth} | To {chat_message.sender_nickname}")
            if retry_depth < config.AI_SCRIPT_MAX_RETRY_TIMES:
                if config.DEBUG_IN_CHAT:
                    await chat_service.send_message(
                        chat_message.chat_key,
                        f"[Debug] ç¨‹å¼è¿è¡Œå‡ºé”™: {err_msg or 'No error message'}\næ­£åœ¨è°ƒè¯•ä¸­...({retry_depth + 1}/{config.AI_SCRIPT_MAX_RETRY_TIMES})",
                    )
                await agent_run(chat_message, addition_prompt_message, retry_depth + 1)
            else:
                await chat_service.send_message(chat_message.chat_key, "ç¨‹å¼è¿è¡Œå‡ºé”™ï¼Œè¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œåœæ­¢é‡è¯•ã€‚")
        else:
            output_msg = result[:100] if result else "No output"
            logger.info(f"ç¨‹å¼æ‰§è¡ŒæˆåŠŸ: {output_msg}... | To {chat_message.sender_nickname}")
            await push_system_message(
                chat_message.chat_key,
                f'"""script:>\n{ret_content}\n"""The requested program was executed successfully, and the output is: {output_msg}...',
            )
            return
        return
