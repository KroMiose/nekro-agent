import asyncio
import os
import time
from typing import List, Optional, Set, Union

import weave

from nekro_agent.core import logger
from nekro_agent.core.config import ModelConfigGroup, config
from nekro_agent.core.os_env import PROMPT_LOG_DIR
from nekro_agent.libs.miose_llm import (
    BaseScene,
    BaseStore,
    ModelResponse,
    Runner,
)
from nekro_agent.libs.miose_llm.clients.chat_openai import (
    OpenAIChatClient,
)
from nekro_agent.libs.miose_llm.components import (
    TextComponent,
)
from nekro_agent.libs.miose_llm.creators.openai import (
    AiMessage,
    ImageMessageSegment,
    OpenAIPromptCreator,
    SystemMessage,
    UserMessage,
)
from nekro_agent.libs.miose_llm.exceptions import (
    ResolveError,
    SceneRuntimeError,
)
from nekro_agent.libs.miose_llm.tools.tokenizers import TikTokenizer
from nekro_agent.models.db_chat_channel import DBChatChannel
from nekro_agent.models.db_chat_message import DBChatMessage
from nekro_agent.schemas.chat_message import ChatMessage, ChatMessageSegmentImage
from nekro_agent.services.chat import chat_service
from nekro_agent.services.sandbox.executor import CODE_RUN_ERROR_FLAG, limited_run_code
from nekro_agent.systems.message.push_bot_msg import push_system_message
from nekro_agent.tools.common_util import (
    compress_image,
    convert_file_name_to_access_path,
    get_downloaded_prompt_file_path,
)

from .components.chat_history_cmp import ChatHistoryComponent
from .components.chat_ret_cmp import (
    ChatResponseResolver,
    ChatResponseType,
    check_negative_response,
)

OPENAI_BASE_URL = os.environ.get("OPENAI_BASE_URL")
OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY")


class ChatScene(BaseScene):
    """åŸºæœ¬å¯¹è¯åœºæ™¯ç±»"""

    class Store(BaseStore):
        """åœºæ™¯æ•°æ®æºç±»"""

        chat_key: str = ""
        chat_preset: str = config.AI_CHAT_PRESET_SETTING
        one_time_code: str = ""


@weave.op(name="agent_run")
async def agent_run(
    chat_message: ChatMessage,
    addition_prompt_message: Optional[List[Union[UserMessage, AiMessage]]] = None,
    retry_depth: int = 0,
):
    """ä»£ç†æ‰§è¡Œå‡½æ•°"""

    sta_timestamp = time.time()
    one_time_code = os.urandom(4).hex()  # é˜²æ­¢æç¤ºè¯æ³¨å…¥ï¼Œç”Ÿæˆä¸€æ¬¡æ€§éšæœºç 

    if not addition_prompt_message:
        addition_prompt_message = []

    logger.info(f"æ­£åœ¨æ„å»ºå¯¹è¯åœºæ™¯: {chat_message.chat_key}")
    if config.DEBUG_IN_CHAT:
        await chat_service.send_message(chat_message.chat_key, "[Debug] æ€è€ƒä¸­ğŸ¤”...")

    db_chat_channel: DBChatChannel = await DBChatChannel.get_channel(chat_key=chat_message.chat_key)
    # logger.info(f"åŠ è½½å¯¹è¯åœºæ™¯é…ç½®: {db_chat_channel.get_channel_data().render_prompt()}")

    # 1. æ„é€ ä¸€ä¸ªåº”ç”¨åœºæ™¯
    scene = ChatScene()
    scene.store.set("chat_key", chat_message.chat_key)
    scene.store.set("one_time_code", one_time_code)

    # 2. æ„å»ºèŠå¤©è®°å½•ç»„ä»¶
    chat_history_component = (
        ChatHistoryComponent(scene)
        .bind(
            param_key="one_time_code",
            store_key="one_time_code",
            src_store=scene.store,
        )
        .bind(
            param_key="chat_key",
            store_key="chat_key",
            src_store=scene.store,
        )
    )
    record_sta_timestamp = int(time.time() - config.AI_CHAT_CONTEXT_EXPIRE_SECONDS)
    recent_chat_messages: List[DBChatMessage] = await (
        DBChatMessage.filter(
            send_timestamp__gte=record_sta_timestamp,
            chat_key=chat_message.chat_key,
        )
        .order_by("-send_timestamp")
        .limit(config.AI_CHAT_CONTEXT_MAX_LENGTH)
    )
    # åè½¬åˆ—è¡¨é¡ºåºå¹¶ç¡®ä¿ä¸è¶…è¿‡æœ€å¤§é•¿åº¦
    recent_chat_messages = recent_chat_messages[::-1][-config.AI_CHAT_CONTEXT_MAX_LENGTH :]

    # æå–å¹¶æ„é€ å›¾ç‰‡ç‰‡æ®µ
    image_segments: List[ChatMessageSegmentImage] = []
    for db_message in recent_chat_messages:
        for seg in db_message.parse_content_data():
            if isinstance(seg, ChatMessageSegmentImage):
                image_segments.append(seg)

    img_seg_prompts: List[Union[str, ImageMessageSegment]] = []
    img_seg_set: Set[str] = set()
    if image_segments and config.AI_ENABLE_VISION:
        img_seg_prompts.append("Here are some images in the chat history:")
        for seg in image_segments[::-1]:
            if len(img_seg_set) >= config.AI_VISION_IMAGE_LIMIT:
                break
            if seg.local_path:
                if seg.file_name in img_seg_set:
                    continue
                access_path = convert_file_name_to_access_path(seg.file_name, chat_message.chat_key)
                img_seg_set.add(seg.file_name)
                # æ£€æŸ¥å›¾ç‰‡å¤§å°
                if access_path.stat().st_size > config.AI_VISION_IMAGE_SIZE_LIMIT_KB * 1024:
                    # å‹ç¼©å›¾ç‰‡
                    try:
                        compressed_path = compress_image(access_path, config.AI_VISION_IMAGE_SIZE_LIMIT_KB)
                    except Exception as e:
                        logger.error(f"å‹ç¼©å›¾ç‰‡æ—¶å‘ç”Ÿé”™è¯¯: {e} | å›¾ç‰‡è·¯å¾„: {access_path} è·³è¿‡å¤„ç†...")
                        continue
                    img_seg_prompts.append(f"<{one_time_code} | Image:{get_downloaded_prompt_file_path(seg.file_name)}>")
                    img_seg_prompts.append(ImageMessageSegment.from_path(str(compressed_path)))
                    logger.info(f"å‹ç¼©å›¾ç‰‡: {access_path.name} -> {compressed_path.stat().st_size / 1024}KB")
                else:
                    img_seg_prompts.append(f"<{one_time_code} | Image:{get_downloaded_prompt_file_path(seg.file_name)}>")
                    img_seg_prompts.append(ImageMessageSegment.from_path(str(access_path)))
            elif seg.remote_url:
                if seg.remote_url in img_seg_set:
                    continue
                img_seg_set.add(seg.remote_url)
                img_seg_prompts.append(f"<{one_time_code} | Image:{seg.remote_url}>")
                img_seg_prompts.append(ImageMessageSegment.from_url(seg.remote_url))

    for db_message in recent_chat_messages:
        chat_history_component.append_chat_message(db_message)
    logger.info(f"åŠ è½½æœ€è¿‘ {len(recent_chat_messages)} æ¡å¯¹è¯è®°å½•")

    # 3. æ„é€  OpenAI æç¤ºè¯
    prompt_creator = OpenAIPromptCreator(
        SystemMessage(
            TextComponent(
                "Base Character Stetting For You: {chat_preset}",
                src_store=scene.store,
            ),
            ChatResponseResolver.example(one_time_code),  # ç”Ÿæˆä¸€ä¸ªè§£æç»“æœç¤ºä¾‹
            sep="\n\n",  # è‡ªå®šä¹‰æ„å»º prompt çš„åˆ†éš”ç¬¦ é»˜è®¤ä¸º "\n"
        ),
        UserMessage(ChatResponseResolver.practice_question_1()),
        AiMessage(ChatResponseResolver.practice_response_1()),
        UserMessage(ChatResponseResolver.practice_question_2()),
        AiMessage(ChatResponseResolver.practice_response_2()),
        UserMessage(
            "Good, this is an effective response to a positive action. Next is a real user conversation scene\n\n",
            *img_seg_prompts,
            TextComponent(
                (
                    f"{(await db_chat_channel.get_channel_data()).render_prompts()}\n"  # èŠå¤©é¢‘é“é…ç½®
                    "Current Chat Key: {chat_key}"  # å½“å‰èŠå¤©ä¼šè¯é”®å
                ),
                src_store=scene.store,
            ),
            chat_history_component,
        ),
        *addition_prompt_message,
        # # æ–‡æœ¬ç”Ÿæˆä½¿ç”¨çš„å‚æ•°
        # temperature=0.3,
        # presence_penalty=0.3,
        # frequency_penalty=0.4,
    )

    # 4. ç»‘å®š LLM æ‰§è¡Œå™¨
    model_group: ModelConfigGroup = ModelConfigGroup.model_validate(config.MODEL_GROUPS[config.USE_MODEL_GROUP])
    scene.attach_runner(  # ä¸ºåœºæ™¯ç»‘å®š LLM æ‰§è¡Œå™¨
        Runner(
            client=OpenAIChatClient(
                model=model_group.CHAT_MODEL,
                api_key=model_group.API_KEY or OPENAI_API_KEY,
                base_url=model_group.BASE_URL or OPENAI_BASE_URL,
                proxy=model_group.CHAT_PROXY,
            ),  # æŒ‡å®šèŠå¤©å®¢æˆ·ç«¯
            tokenizer=TikTokenizer(model=model_group.CHAT_MODEL),  # æŒ‡å®šåˆ†è¯å™¨
            prompt_creator=prompt_creator,
        ),
    )

    # 5. è·å–ç»“æœä¸è§£æ
    for _ in range(config.AI_CHAT_LLM_API_MAX_RETRIES):
        try:
            logger.debug("å‘é€ç”Ÿæˆè¯·æ±‚...")
            scene_run_sta_timestamp = time.time()
            mr: ModelResponse = await scene.run()
            logger.debug(f"LLM è¿è¡Œè€—æ—¶: {time.time() - scene_run_sta_timestamp:.3f}s")
            break
        except Exception as e:
            logger.error(f"LLM API error: {e}")
            await asyncio.sleep(1)
    else:
        await chat_service.send_agent_message(chat_message.chat_key, "å“å‘€ï¼Œè¯·æ±‚æ¨¡å‹å‘ç”Ÿäº†æœªçŸ¥é”™è¯¯ï¼Œç­‰ä¼šå„¿å†è¯•è¯•å§ ~")
        raise SceneRuntimeError("LLM API error: è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œåœæ­¢é‡è¯•ã€‚") from None

    if (not retry_depth) and check_negative_response(mr.response_text):
        logger.warning(f"æ£€æµ‹åˆ°æ¶ˆæå›å¤: {mr.response_text}ï¼Œæ‹’ç»ç»“æœå¹¶é‡è¯•")
        if config.DEBUG_IN_CHAT:
            await chat_service.send_message(chat_message.chat_key, "[Debug] æ£€æµ‹åˆ°æ¶ˆæå›å¤ï¼Œæ‹’ç»ç»“æœå¹¶é‡è¯•...")
        addition_prompt_message.append(AiMessage(mr.response_text))
        addition_prompt_message.append(
            UserMessage(
                "[System Automatic Detection] A suspected negative or invalid response is detected in your reply (such as asking for a meaningless wait or claiming to do something but not do anything). Your answers must be consistent with your words and deeds, no pretending behavior, and no meaningless promises. If you think this is an error, please ** keep the previously agreed reply format ** and try again.",
            ),
        )
        await agent_run(chat_message, addition_prompt_message, retry_depth + 1)
        return

    try:
        resolved_response: ChatResponseResolver = ChatResponseResolver.resolve(
            model_response=mr,
        )  # ä½¿ç”¨æŒ‡å®šè§£æå™¨è§£æç»“æœ
        logger.debug("è§£æå®Œæˆç»“æœå®Œæˆ")
    except Exception as e:
        logger.error(f"è§£æç»“æœå‡ºé”™: {e}")
        raise ResolveError(f"è§£æç»“æœå‡ºé”™: {e}") from e

    # 7. æ‰§è¡Œå“åº”ç»“æœ
    logger.debug(f"å¼€å§‹æ‰§è¡Œ {len(resolved_response.ret_list)} æ¡å“åº”ç»“æœ")
    for ret_data in resolved_response.ret_list:
        # æœ€ç»ˆè¿‡æ»¤ä¸€æ¬¡å¾…æ‰§è¡Œçš„ä»£ç 
        if ret_data.content.lower().startswith("```python"):
            ret_data.content = ret_data.content[10:]
        if ret_data.content.lower().endswith("```"):
            ret_data.content = ret_data.content[:-3]
        await agent_exec_result(ret_data.type, ret_data.content, chat_message, addition_prompt_message, retry_depth)

    # 8. åé¦ˆä¸ä¿å­˜æ•°æ®
    if config.SAVE_PROMPTS_LOG:
        current_strftime = time.strftime("%Y%m%d%H%M%S")
        logger.debug(f"ä¿å­˜å¯¹è¯è®°å½•: {current_strftime}")
        mr.save(
            prompt_file=f".temp/prompts/chat_prompt-{current_strftime}.txt",
            response_file=f".temp/prompts/chat_response-{current_strftime}.json",
        )
        logger.debug("å¦å­˜æœ€æ–°å¯¹è¯è®°å½•")
        mr.save(
            prompt_file=".temp/chat_prompt-latest.txt",
            response_file=".temp/chat_response-latest.json",
        )

    logger.info(f"æœ¬è½®å“åº”è€—æ—¶: {time.time() - sta_timestamp:.2f}s | To {chat_message.sender_nickname}")


async def agent_exec_result(
    ret_type: ChatResponseType,
    ret_content: str,
    chat_message: ChatMessage,
    addition_prompt_message: List[Union[UserMessage, AiMessage]],
    retry_depth: int = 0,
):
    if ret_type is ChatResponseType.TEXT:
        logger.info(f"è§£ææ–‡æœ¬å›å¤: {ret_content} | To {chat_message.sender_nickname}")
        await chat_service.send_agent_message(chat_message.chat_key, ret_content, record=True)
        return

    if ret_type is ChatResponseType.SCRIPT:
        if ret_content.endswith("\n```"):
            ret_content = ret_content[:-3]
        logger.info(f"è§£æç¨‹å¼å›å¤: ç­‰å¾…æ‰§è¡Œèµ„æº | To {chat_message.sender_nickname}")
        if config.DEBUG_IN_CHAT:
            await chat_service.send_message(chat_message.chat_key, "[Debug] æ‰§è¡Œç¨‹å¼ä¸­ğŸ–¥ï¸...")
        result: str = await limited_run_code(ret_content, from_chat_key=chat_message.chat_key)
        if result.endswith(CODE_RUN_ERROR_FLAG):  # è¿è¡Œå‡ºé”™æ ‡è®°ï¼Œå°†é”™è¯¯ä¿¡æ¯è¿”å›ç»™ AI
            err_msg = result[: -len(CODE_RUN_ERROR_FLAG)]
            addition_prompt_message.append(AiMessage(f"{ret_content}"))
            if retry_depth < config.AI_SCRIPT_MAX_RETRY_TIMES - 1:
                addition_prompt_message.append(
                    UserMessage(
                        f"Code run error: {err_msg or 'No error message'}\nPlease maintain agreed reply format and try again.",
                    ),
                )
            else:
                addition_prompt_message.append(
                    UserMessage(
                        f"Code run error: {err_msg or 'No error message'}\nThe number of retries has reached the limit, you should give up retries and explain the problem you are experiencing.",
                    ),
                )
            logger.info(f"ç¨‹å¼è¿è¡Œå‡ºé”™: ...{err_msg[-100:]} | é‡è¯•æ¬¡æ•°: {retry_depth} | To {chat_message.sender_nickname}")
            if retry_depth < config.AI_SCRIPT_MAX_RETRY_TIMES:
                if config.DEBUG_IN_CHAT:
                    await chat_service.send_message(
                        chat_message.chat_key,
                        f"[Debug] ç¨‹å¼è¿è¡Œå‡ºé”™: {err_msg or 'No error message'}\næ­£åœ¨è°ƒè¯•ä¸­...({retry_depth + 1}/{config.AI_SCRIPT_MAX_RETRY_TIMES})",
                    )
                await agent_run(chat_message, addition_prompt_message, retry_depth + 1)
            else:
                await chat_service.send_message(chat_message.chat_key, "ç¨‹å¼è¿è¡Œå‡ºé”™ï¼Œè¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œåœæ­¢é‡è¯•ã€‚")
        else:
            output_msg = result[:100] if result else "No output"
            logger.info(f"ç¨‹å¼æ‰§è¡ŒæˆåŠŸ: {output_msg}... | To {chat_message.sender_nickname}")
            await push_system_message(
                chat_message.chat_key,
                f'"""python(history run)\n{ret_content}\n"""The requested program was executed successfully, and the output is: {output_msg}...',
            )
            return
        return
